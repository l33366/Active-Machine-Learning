{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-17T15:41:05.202903Z","iopub.execute_input":"2022-11-17T15:41:05.203310Z","iopub.status.idle":"2022-11-17T15:41:05.212637Z","shell.execute_reply.started":"2022-11-17T15:41:05.203275Z","shell.execute_reply":"2022-11-17T15:41:05.211308Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/emotions-dataset-for-nlp/val.txt\n/kaggle/input/emotions-dataset-for-nlp/test.txt\n/kaggle/input/emotions-dataset-for-nlp/train.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install text_hammer\n!pip install transformers\nimport warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab\nimport text_hammer as th\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom array import *\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\nfrom transformers import BertTokenizer,TFBertModel\n#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nbert = TFBertModel.from_pretrained('bert-base-cased')\n\ntokenizer.save_pretrained('bert-tokenizer')\nbert.save_pretrained('bert-model')\nimport shutil\nshutil.make_archive('bert-tokenizer', 'zip', 'bert-tokenizer')\nshutil.make_archive('bert-model','zip','bert-model')\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\ndef batch_selector(dataset,samples_no):\n    training_batch=dataset.iloc[:samples_no]\n    unlabelled_batch=dataset.iloc[samples_no:]\n    return training_batch, unlabelled_batch\ndef text_preprocessing(df,col_name):\n    column = col_name\n    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n    df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_stopwords(x))\n\n    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n#     df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n    return(df)\n\ndef num_sim(n1, n2):\n  \"\"\" calculates a similarity score between 2 numbers \"\"\"\n  return 1 - abs(n1 - n2) / (n1 + n2)\n\ndef maxConflictFinder(array):\n    a_list = list(array)\n    are= list(array)\n    max_value = max(a_list)\n    max_index = a_list.index(max_value)\n    maxele=a_list.pop(max_index)\n    for i in range(len(a_list)):\n        intdiff=num_sim(a_list[i],maxele)\n        if((intdiff>0.95)):\n            return are.index(a_list[i]),are.index(maxele)\n        else:\n            return None\n\ndef indexGenerator(df):\n    n,_=df.shape\n    index=np.arange(0,n)\n    df['index']=index\n    df=df.set_index('index')\n    return df\n\ndef dataTx(n,df_test1,df_train1):\n    df_train1=df_trai1n.append(df_test1[n:n+1])\n    df_test1=df_test1.drop(n)\n    return df_test1,df_train1\n\nimport tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')\n\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\ndbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:41:06.430796Z","iopub.execute_input":"2022-11-17T15:41:06.431756Z","iopub.status.idle":"2022-11-17T15:43:09.589104Z","shell.execute_reply.started":"2022-11-17T15:41:06.431717Z","shell.execute_reply":"2022-11-17T15:43:09.588089Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting text_hammer\n  Downloading text_hammer-0.1.5-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from text_hammer) (1.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from text_hammer) (1.21.6)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from text_hammer) (3.3.1)\nRequirement already satisfied: TextBlob in /opt/conda/lib/python3.7/site-packages (from text_hammer) (0.17.1)\nCollecting beautifulsoup4==4.9.1\n  Downloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4==4.9.1->text_hammer) (2.3.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->text_hammer) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->text_hammer) (2022.1)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (0.6.2)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (0.4.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (59.8.0)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (8.0.17)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (1.0.8)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (3.1.2)\nCollecting typing-extensions<4.2.0,>=3.7.4\n  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (4.64.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (21.3)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (2.0.8)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (3.3.0)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (0.7.8)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (3.0.10)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (1.8.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (2.0.6)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (1.0.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (3.0.7)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (2.28.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (2.4.4)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy->text_hammer) (0.10.1)\nRequirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.7/site-packages (from TextBlob->text_hammer) (3.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy->text_hammer) (3.8.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk>=3.1->TextBlob->text_hammer) (1.0.1)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk>=3.1->TextBlob->text_hammer) (2021.11.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk>=3.1->TextBlob->text_hammer) (8.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy->text_hammer) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy->text_hammer) (5.2.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->text_hammer) (1.15.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (2022.6.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy->text_hammer) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk>=3.1->TextBlob->text_hammer) (4.12.0)\nInstalling collected packages: typing-extensions, beautifulsoup4, text_hammer\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Uninstalling typing_extensions-4.3.0:\n      Successfully uninstalled typing_extensions-4.3.0\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.11.1\n    Uninstalling beautifulsoup4-4.11.1:\n      Successfully uninstalled beautifulsoup4-4.11.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.0 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nflax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.12.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.72 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed beautifulsoup4-4.9.1 text_hammer-0.1.5 typing-extensions-4.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2022-11-17 15:41:53.003578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.004629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.005842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.006695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.007495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.008340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.011809: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-17 15:41:53.262697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.263531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.264270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.264985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.265678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:41:53.266397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.069051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.069970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.070716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.071421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.072191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.072851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12839 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-11-17 15:42:05.076169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 15:42:05.076886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 12839 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"741b53e6af2542e69c7b6fe36c353b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed81b2015b741699bf0783e0f4db023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f5042abbff46ca93d7d54680882521"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ddf7d63da1540cfbb12b36630812011"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f780f0fa9c47fcae9e0d9c3fd35b29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df2f3fcf5c0c486a82f35e99aac0c1ff"}},"metadata":{}},{"name":"stderr","text":"2022-11-17 15:43:09.092904: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/emotions-dataset-for-nlp/train.txt', header =None, sep =';', names = ['Input','Sentiment'], encoding='utf-8')\ndf_test = pd.read_csv('../input/emotions-dataset-for-nlp/test.txt', header = None, sep =';', names = ['Input','Sentiment'],encoding='utf-8')\ndf_val=pd.read_csv('../input/emotions-dataset-for-nlp/val.txt',header=None,sep=';',names=['Input','Sentiment'],encoding='utf-8')\ntweet_df = pd.concat([df_train,df_test,df_val], axis = 0)\ndf_surprise=tweet_df[tweet_df['Sentiment']=='surprise']\ndf_fear    =tweet_df[tweet_df['Sentiment']=='fear']\ndf_joy     =tweet_df[tweet_df['Sentiment']=='joy']\ndf_love    =tweet_df[tweet_df['Sentiment']=='love']\ndf_anger   =tweet_df[tweet_df['Sentiment']=='anger']\ndf_sadness =tweet_df[tweet_df['Sentiment']=='sadness']\n\nprint(f'sadness:{df_sadness.shape}')\nprint(f'anger:{df_anger.shape}')\nprint(f'love:{df_love.shape}')\nprint(f'joy:{df_joy.shape}')\nprint(f'fear:{df_fear.shape}')\nprint(f'surprise:{df_surprise.shape}')\n\nNo_samples=100\nsurprise_train,surprise_test=batch_selector(df_surprise,No_samples)\nfear_train,fear_test=batch_selector(df_fear,No_samples)\njoy_train,joy_test=batch_selector(df_joy,No_samples)\nlove_train,love_test=batch_selector(df_love,No_samples)\nanger_train,anger_test=batch_selector(df_anger,No_samples)\nsadness_train,sadness_test=batch_selector(df_sadness,No_samples)\n\ntest250 = pd.concat([surprise_test,fear_test,joy_test,love_test,anger_test,sadness_test], axis = 0)\ntrain250= pd.concat([surprise_train,fear_train,joy_train,love_train,anger_train,sadness_train], axis = 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:43:59.311442Z","iopub.execute_input":"2022-11-17T15:43:59.312606Z","iopub.status.idle":"2022-11-17T15:43:59.432401Z","shell.execute_reply.started":"2022-11-17T15:43:59.312557Z","shell.execute_reply":"2022-11-17T15:43:59.431357Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"sadness:(5797, 2)\nanger:(2709, 2)\nlove:(1641, 2)\njoy:(6761, 2)\nfear:(2373, 2)\nsurprise:(719, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test,df_train=test250,train250\nprint(df_test.shape,df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:44:01.838883Z","iopub.execute_input":"2022-11-17T15:44:01.839288Z","iopub.status.idle":"2022-11-17T15:44:01.845613Z","shell.execute_reply.started":"2022-11-17T15:44:01.839252Z","shell.execute_reply":"2022-11-17T15:44:01.844456Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(19400, 2) (600, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# for k in range(100):\n#     print('####################',k+1,\"###########################\")\n#     df_cleaned_train_20 = text_preprocessing(df_train,'Input')\n#     df_cleaned_test_20 = text_preprocessing(df_test,'Input')\n#     train20_df = df_cleaned_train_20.copy()\n#     test20_df = df_cleaned_test_20.copy()\n#     train20_df['num_words'] = train20_df.Input.apply(lambda x:len(x.split()))\n#     test20_df['num_words'] = test20_df.Input.apply(lambda x:len(x.split()))\n#     train20_df['Sentiment'] = train20_df.Sentiment.astype('category')\n#     test20_df['Sentiment'] = test20_df.Sentiment.astype('category')\n#     train20_df['Sentiment']  =  train20_df.Sentiment.cat.codes\n#     test20_df['Sentiment']  =  test20_df.Sentiment.cat.codes\n#     from sklearn.model_selection import train_test_split\n#     data_train,data_test = train_test_split(train20_df, test_size = 0.30, random_state = 42, stratify = train20_df.Sentiment)\n#     surprise_string ='i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies '\n#     len(surprise_string)\n#     res = len(surprise_string.split())\n#     print(\"The number of words in string are : \" + str(res),len(surprise_string) )\n#     data_test = data_test.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n#     data_train = data_train.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n#     data_val = test20_df.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n    \n#     x_train = tokenizer(\n#         text=data_train.Input.tolist(),\n#         add_special_tokens=True,\n#         max_length=70,\n#         truncation=True,\n#         padding=True, \n#         return_tensors='tf',\n#         return_token_type_ids = False,\n#         return_attention_mask = True,\n#         verbose = True)\n    \n#     x_test = tokenizer(\n#         text=data_test.Input.tolist(),\n#         add_special_tokens=True,\n#         max_length=70,\n#         truncation=True,\n#         padding=True, \n#         return_tensors='tf',\n#         return_token_type_ids = False,\n#         return_attention_mask = True,\n#         verbose = True)\n    \n#     x_val = tokenizer(\n#         text=data_val.Input.tolist(),\n#         add_special_tokens=True,\n#         max_length=70,\n#         truncation=True,\n#         padding=True, \n#         return_tensors='tf',\n#         return_token_type_ids = False,\n#         return_attention_mask = True,\n#         verbose = True)\n    \n#     max_len = 70\n#     import tensorflow as tf\n#     from tensorflow.keras.layers import Input, Dense\n\n#     input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n#     input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n#     embeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\n#     out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n#     out = Dense(128, activation='relu')(out)\n#     out = tf.keras.layers.Dropout(0.1)(out)\n#     out = Dense(32,activation = 'relu')(out)\n#     y = Dense(6,activation = 'sigmoid')(out)\n#     model4 = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n#     model4.layers[2].trainable = True\n    \n#     optimizer = Adam(\n#         learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n#         epsilon=1e-08,\n#         decay=0.01,\n#         clipnorm=1.0)\n#     loss =CategoricalCrossentropy(from_logits = True)\n#     metric = CategoricalAccuracy('balanced_accuracy'),\n#     model4.compile(\n#         optimizer = optimizer,\n#         loss = loss, \n#         metrics = metric)\n#     tf.config.experimental_run_functions_eagerly(True)\n#     tf.config.run_functions_eagerly(True)\n#     train_history = model4.fit(\n#         x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n#         y = to_categorical(data_train.Sentiment),\n#         validation_data = (\n#             {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, to_categorical(data_test.Sentiment)\n#             ),\n#        epochs=10,\n#        batch_size=36\n#        )\n#     predicted_raw = model4.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})\n#     ine,_=predicted_raw.shape\n#     lis=[]\n#     lis3=[]\n#     df_test=indexGenerator(df_test)\n#     df_train=indexGenerator(df_train)\n#     for i in range(ine):\n#         if (maxConflictFinder(predicted_raw[i]) != None):\n#             lis.append(maxConflictFinder(predicted_raw[i]))\n#             lis3.append(i)\n            \n#     df_test=indexGenerator(df_test)\n#     df_train=indexGenerator(df_train)\n    \n#     for i in lis3:\n#         df_test,df_train=dataTx(i,df_test,df_train)\n    \n#     print(df_test.shape,df_train.shape)\n#     df_test=indexGenerator(df_test)\n#     df_train=indexGenerator(df_train)\n\n#     df_cleaned_train_20 = text_preprocessing(df_train,'Input')\n#     df_cleaned_test_20 = text_preprocessing(df_test,'Input')\n#     train20_df = df_cleaned_train_20.copy()\n#     test20_df = df_cleaned_test_20.copy()\n#     train20_df['num_words'] = train20_df.Input.apply(lambda x:len(x.split()))\n#     test20_df['num_words'] = test20_df.Input.apply(lambda x:len(x.split()))\n#     train20_df['Sentiment'] = train20_df.Sentiment.astype('category')\n#     test20_df['Sentiment'] = test20_df.Sentiment.astype('category')\n#     train20_df['Sentiment']  =  train20_df.Sentiment.cat.codes\n#     test20_df['Sentiment']  =  test20_df.Sentiment.cat.codes\n\n#     from sklearn.model_selection import train_test_split\n#     data_train,data_test = train_test_split(train20_df, test_size = 0.30, random_state = 42, stratify = train20_df.Sentiment)\n\n#     surprise_string ='i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies '\n#     len(surprise_string)\n#     res = len(surprise_string.split())\n#     print(\"The number of words in string are : \" + str(res),len(surprise_string) )\n#     data_test = data_test.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n#     data_train = data_train.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n#     data_val = test20_df.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n    \n#     x_train = tokenizer(\n#         text=data_train.Input.tolist(),\n#         add_special_tokens=True,\n#         max_length=70,\n#         truncation=True,\n#         padding=True, \n#         return_tensors='tf',\n#         return_token_type_ids = False,\n#         return_attention_mask = True,\n#         verbose = True)\n\n#     x_test = tokenizer(\n#         text=data_test.Input.tolist(),\n#         add_special_tokens=True,\n#         max_length=70,\n#         truncation=True,\n#         padding=True, \n#         return_tensors='tf',\n#         return_token_type_ids = False,\n#         return_attention_mask = True,\n#         verbose = True)\n\n#     x_val = tokenizer(\n#         text=data_val.Input.tolist(),\n#         add_special_tokens=True,\n#         max_length=70,\n#         truncation=True,\n#         padding=True, \n#         return_tensors='tf',\n#         return_token_type_ids = False,\n#         return_attention_mask = True,\n#         verbose = True)\n    \n#     max_len = 70\n#     import tensorflow as tf\n#     from tensorflow.keras.layers import Input, Dense\n\n#     input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n#     input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n#     embeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\n#     out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n#     out = Dense(128, activation='relu')(out)\n#     out = tf.keras.layers.Dropout(0.1)(out)\n#     out = Dense(32,activation = 'relu')(out)\n\n#     y = Dense(6,activation = 'sigmoid')(out)\n    \n#     model5 = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n#     model5.layers[2].trainable = True\n#     optimizer = Adam(\n#         learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n#         epsilon=1e-08,\n#         decay=0.01,\n#         clipnorm=1.0)\n#     loss =CategoricalCrossentropy(from_logits = True)\n#     metric = CategoricalAccuracy('balanced_accuracy'),\n#     model5.compile(\n#         optimizer = optimizer,\n#         loss = loss, \n#         metrics = metric)\n#     tf.config.experimental_run_functions_eagerly(True)\n#     tf.config.run_functions_eagerly(True)\n\n#     train_history = model5.fit(\n#         x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n#         y = to_categorical(data_train.Sentiment),\n#         validation_data = (\n#             {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, to_categorical(data_test.Sentiment)\n#             ),\n#        epochs=10,\n#        batch_size=36\n#        )\n#     predicted_raw = model5.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})\n#     predicted_raw = model5.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})\n#     y_predicted = np.argmax(predicted_raw, axis = 1)\n#     print(classification_report(data_val.Sentiment, y_predicted))\n#     print(confusion_matrix(data_val.Sentiment,y_predicted))\n#     cf_matrix=confusion_matrix(data_val.Sentiment,y_predicted)\n#     if(len(lis3)==0):\n#         break\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:44:03.238624Z","iopub.execute_input":"2022-11-17T15:44:03.239007Z","iopub.status.idle":"2022-11-17T15:44:03.253071Z","shell.execute_reply.started":"2022-11-17T15:44:03.238973Z","shell.execute_reply":"2022-11-17T15:44:03.252028Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def dataTx_trial(txlist,df_test1,df_train1):\n    for n in txlist:\n        df_train1=df_train1.append(indexGenerator((df_test1.iloc[n].to_frame()).transpose()))\n    for k in txlist:\n        df_test1=df_test1.drop(k)\n    return df_test1,df_train1\n\ndf_test,df_train=test250,train250\nprint(df_test.shape,df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:44:04.764169Z","iopub.execute_input":"2022-11-17T15:44:04.765197Z","iopub.status.idle":"2022-11-17T15:44:04.773250Z","shell.execute_reply.started":"2022-11-17T15:44:04.765128Z","shell.execute_reply":"2022-11-17T15:44:04.772210Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(19400, 2) (600, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"ine,_=predicted_raw.shape\nlis=[]\nlis3=[]\ndf_test=indexGenerator(df_test)\ndf_train=indexGenerator(df_train)\n\nfor i in range(ine):\n    if (maxConflictFinder(predicted_raw[i]) != None):\n        lis3.append(i)\n    else:\n        lis.append(i)\n        \nprint(len(lis3))\ndf_test=indexGenerator(df_test)\ndf_train=indexGenerator(df_train)\n        \ndf_test,df_train=dataTx_trial(lis3,df_test,df_train)\n    \ndf_test=indexGenerator(df_test)\ndf_train=indexGenerator(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.shape,df_train.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:44:11.431464Z","iopub.execute_input":"2022-11-17T15:44:11.431877Z","iopub.status.idle":"2022-11-17T15:44:11.440839Z","shell.execute_reply.started":"2022-11-17T15:44:11.431845Z","shell.execute_reply":"2022-11-17T15:44:11.439455Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(19400, 2) (600, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_cleaned_train_20 = text_preprocessing(df_train,'Input')\ndf_cleaned_test_20 = text_preprocessing(df_test,'Input')\ntrain20_df = df_cleaned_train_20.copy()\ntest20_df = df_cleaned_test_20.copy()\ntrain20_df['num_words'] = train20_df.Input.apply(lambda x:len(x.split()))\ntest20_df['num_words'] = test20_df.Input.apply(lambda x:len(x.split()))\ntrain20_df['Sentiment'] = train20_df.Sentiment.astype('category')\ntest20_df['Sentiment'] = test20_df.Sentiment.astype('category')\ntrain20_df['Sentiment']  =  train20_df.Sentiment.cat.codes\ntest20_df['Sentiment']  =  test20_df.Sentiment.cat.codes\n\nfrom sklearn.model_selection import train_test_split\ndata_train,data_test = train_test_split(train20_df, test_size = 0.30, random_state = 42, stratify = train20_df.Sentiment)\n\nsurprise_string ='i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies '\nlen(surprise_string)\nres = len(surprise_string.split())\nprint(\"The number of words in string are : \" + str(res),len(surprise_string) )\ndata_test = data_test.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\ndata_train = data_train.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\ndata_val = test20_df.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n\nx_train = tokenizer(\n    text=data_train.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\nx_test = tokenizer(\n    text=data_test.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\nx_val = tokenizer(\n    text=data_val.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\nmax_len = 70\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\n\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\nembeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\n\ny = Dense(6,activation = 'sigmoid')(out)\n    \nmodel5 = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel5.layers[2].trainable = True\n\noptimizer = Adam(\n    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss =CategoricalCrossentropy(from_logits = True)\nmetric = CategoricalAccuracy('balanced_accuracy'),\n# Compile the model\nmodel5.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)\n\ntf.config.experimental_run_functions_eagerly(True)\ntf.config.run_functions_eagerly(True)\n\ntrain_history = model5.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = to_categorical(data_train.Sentiment),\n    validation_data = (\n    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, to_categorical(data_test.Sentiment)\n    ),\n  epochs=10,\n    batch_size=36\n)\n\npredicted_raw = model5.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})\ny_predicted = np.argmax(predicted_raw, axis = 1)\nprint(classification_report(data_val.Sentiment, y_predicted))\nprint(confusion_matrix(data_val.Sentiment,y_predicted))\ncf_matrix=confusion_matrix(data_val.Sentiment,y_predicted)","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:44:13.084953Z","iopub.execute_input":"2022-11-17T15:44:13.085772Z","iopub.status.idle":"2022-11-17T15:49:44.941345Z","shell.execute_reply.started":"2022-11-17T15:44:13.085734Z","shell.execute_reply":"2022-11-17T15:49:44.940225Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99fb17bbddcd42ddaaa01eba6df8934e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac151fa3b3154c909632e35e8ef6cea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e310cc54dc54224abda3620cf686357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaccf12ebfc94e27b8026053a7bf5547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57b800f74d344a62940c3e5544cdbaec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c61fa45c79840478931d53a2ec4f9c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b4f2b6857c40e890390ad518acf9c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07701a2eedf74e8ab5e0d682cd507994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36fcea82c52c4312897ed62705311682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66fcd638e74444aa95d9c3fe3e4834a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1faa7b9ee5a4439bb5f7a76a0460bc01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19400 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"295d4d6751fe4dd2b716c48a245308bf"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\n","output_type":"stream"},{"name":"stderr","text":"2022-11-17 15:45:00.984737: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n12/12 [==============================] - 9s 725ms/step - loss: 1.8898 - balanced_accuracy: 0.1924 - val_loss: 1.7950 - val_balanced_accuracy: 0.2099\nEpoch 2/10\n12/12 [==============================] - 9s 760ms/step - loss: 1.6900 - balanced_accuracy: 0.2945 - val_loss: 1.6899 - val_balanced_accuracy: 0.3039\nEpoch 3/10\n12/12 [==============================] - 9s 755ms/step - loss: 1.4873 - balanced_accuracy: 0.4632 - val_loss: 1.5227 - val_balanced_accuracy: 0.4365\nEpoch 4/10\n12/12 [==============================] - 10s 807ms/step - loss: 1.1878 - balanced_accuracy: 0.6105 - val_loss: 1.3096 - val_balanced_accuracy: 0.5249\nEpoch 5/10\n12/12 [==============================] - 9s 789ms/step - loss: 0.8554 - balanced_accuracy: 0.7767 - val_loss: 1.1151 - val_balanced_accuracy: 0.5856\nEpoch 6/10\n12/12 [==============================] - 9s 760ms/step - loss: 0.5265 - balanced_accuracy: 0.9002 - val_loss: 0.9322 - val_balanced_accuracy: 0.6685\nEpoch 7/10\n12/12 [==============================] - 9s 796ms/step - loss: 0.2957 - balanced_accuracy: 0.9572 - val_loss: 0.8015 - val_balanced_accuracy: 0.6851\nEpoch 8/10\n12/12 [==============================] - 9s 776ms/step - loss: 0.1583 - balanced_accuracy: 0.9857 - val_loss: 0.7760 - val_balanced_accuracy: 0.7514\nEpoch 9/10\n12/12 [==============================] - 9s 760ms/step - loss: 0.0726 - balanced_accuracy: 0.9976 - val_loss: 0.7541 - val_balanced_accuracy: 0.7459\nEpoch 10/10\n12/12 [==============================] - 9s 774ms/step - loss: 0.0416 - balanced_accuracy: 0.9929 - val_loss: 0.7802 - val_balanced_accuracy: 0.7569\n              precision    recall  f1-score   support\n\n           0       0.61      0.70      0.65      2609\n           1       0.68      0.74      0.71      2274\n           2       0.90      0.65      0.75      6661\n           3       0.42      0.64      0.50      1541\n           4       0.76      0.74      0.75      5697\n           5       0.47      0.91      0.62       619\n\n    accuracy                           0.70     19401\n   macro avg       0.64      0.73      0.67     19401\nweighted avg       0.74      0.70      0.71     19401\n\n[[1830  335   28   70  330   16]\n [  81 1685   22   25  234  227]\n [ 114  149 4331 1165  623  279]\n [  67   23  295  985  149   22]\n [ 886  282  131  113 4205   80]\n [   8   15   19    6    9  562]]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test,df_train=test250,train250\nprint(df_test.shape,df_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(100):\n    print('####################',k+1,\"###########################\")\n    ine,_=predicted_raw.shape\n    lis=[]\n    lis3=[]\n    df_test=indexGenerator(df_test)\n    df_train=indexGenerator(df_train)\n    for i in range(ine):\n        if (maxConflictFinder(predicted_raw[i]) != None):\n            lis3.append(i)\n    print(len(lis3))        \n    df_test=indexGenerator(df_test)\n    df_train=indexGenerator(df_train)\n    df_test,df_train=dataTx_trial(lis3,df_test,df_train)\n    print(df_test.shape,df_train.shape)\n    df_test=indexGenerator(df_test)\n    df_train=indexGenerator(df_train)\n\n    df_cleaned_train_20 = text_preprocessing(df_train,'Input')\n    df_cleaned_test_20 = text_preprocessing(df_test,'Input')\n    train20_df = df_cleaned_train_20.copy()\n    test20_df = df_cleaned_test_20.copy()\n    train20_df['num_words'] = train20_df.Input.apply(lambda x:len(x.split()))\n    test20_df['num_words'] = test20_df.Input.apply(lambda x:len(x.split()))\n    train20_df['Sentiment'] = train20_df.Sentiment.astype('category')\n    test20_df['Sentiment'] = test20_df.Sentiment.astype('category')\n    train20_df['Sentiment']  =  train20_df.Sentiment.cat.codes\n    test20_df['Sentiment']  =  test20_df.Sentiment.cat.codes\n\n    from sklearn.model_selection import train_test_split\n    data_train,data_test = train_test_split(train20_df, test_size = 0.30, random_state = 42, stratify = train20_df.Sentiment)\n\n    surprise_string ='i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies '\n    len(surprise_string)\n    res = len(surprise_string.split())\n    print(\"The number of words in string are : \" + str(res),len(surprise_string) )\n    data_test = data_test.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n    data_train = data_train.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n    data_val = test20_df.append({'Input':surprise_string, 'Sentiment':1, 'num_words':115}, ignore_index=True)\n    \n    x_train = tokenizer(\n        text=data_train.Input.tolist(),\n        add_special_tokens=True,\n        max_length=70,\n        truncation=True,\n        padding=True, \n        return_tensors='tf',\n        return_token_type_ids = False,\n        return_attention_mask = True,\n        verbose = True)\n\n    x_test = tokenizer(\n        text=data_test.Input.tolist(),\n        add_special_tokens=True,\n        max_length=70,\n        truncation=True,\n        padding=True, \n        return_tensors='tf',\n        return_token_type_ids = False,\n        return_attention_mask = True,\n        verbose = True)\n\n    x_val = tokenizer(\n        text=data_val.Input.tolist(),\n        add_special_tokens=True,\n        max_length=70,\n        truncation=True,\n        padding=True, \n        return_tensors='tf',\n        return_token_type_ids = False,\n        return_attention_mask = True,\n        verbose = True)\n    \n    max_len = 70\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input, Dense\n\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    embeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\n    out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n    out = Dense(128, activation='relu')(out)\n    out = tf.keras.layers.Dropout(0.1)(out)\n    out = Dense(32,activation = 'relu')(out)\n\n    y = Dense(6,activation = 'sigmoid')(out)\n    \n    model5 = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n    model5.layers[2].trainable = True\n    optimizer = Adam(\n        learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n        epsilon=1e-08,\n        decay=0.01,\n        clipnorm=1.0)\n    loss =CategoricalCrossentropy(from_logits = True)\n    metric = CategoricalAccuracy('balanced_accuracy'),\n    model5.compile(\n        optimizer = optimizer,\n        loss = loss, \n        metrics = metric)\n    tf.config.experimental_run_functions_eagerly(True)\n    tf.config.run_functions_eagerly(True)\n\n    train_history = model5.fit(\n        x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n        y = to_categorical(data_train.Sentiment),\n        validation_data = (\n            {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, to_categorical(data_test.Sentiment)\n            ),\n       epochs=10,\n       batch_size=36\n       )\n    predicted_raw = model5.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})\n    predicted_raw = model5.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})\n    y_predicted = np.argmax(predicted_raw, axis = 1)\n    print(classification_report(data_val.Sentiment, y_predicted))\n    print(confusion_matrix(data_val.Sentiment,y_predicted))\n    cf_matrix=confusion_matrix(data_val.Sentiment,y_predicted)\n    if(len(lis3)==0):\n        break\n","metadata":{"execution":{"iopub.status.busy":"2022-11-17T15:50:45.636545Z","iopub.execute_input":"2022-11-17T15:50:45.637321Z","iopub.status.idle":"2022-11-17T17:53:43.622477Z","shell.execute_reply.started":"2022-11-17T15:50:45.637282Z","shell.execute_reply":"2022-11-17T17:53:43.620453Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"#################### 1 ###########################\n2187\n(17213, 2) (2787, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2787 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c8cc9f13d45422db294fc648ebb9d0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2787 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5160f3489744deb3f0807e82c2e7f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2787 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a41852550a44c5b636516c00ec712c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2787 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423a78f8f7d540aaa3b923e205dea623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2787 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad5d26b75b6a43b094297bcfaa4b2c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2787 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2ca6d97c5048e4a7441934a3ece704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17213 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40db042a7fff4535a1313ebfdc518f58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17213 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aeb204806214ba193009a5241158571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17213 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25efa2da41c343d6814a4326c6897e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17213 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038cac7d623b437691b210e9f7f587ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17213 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c750fb065e94ef9acb1c3d60070ca8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17213 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f7a70f78274a0a9a2a41cd7d8748e6"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n55/55 [==============================] - 42s 765ms/step - loss: 0.8058 - balanced_accuracy: 0.7207 - val_loss: 0.3740 - val_balanced_accuracy: 0.8938\nEpoch 2/10\n55/55 [==============================] - 42s 763ms/step - loss: 0.2997 - balanced_accuracy: 0.9042 - val_loss: 0.3902 - val_balanced_accuracy: 0.8866\nEpoch 3/10\n55/55 [==============================] - 41s 756ms/step - loss: 0.1583 - balanced_accuracy: 0.9518 - val_loss: 0.3478 - val_balanced_accuracy: 0.8902\nEpoch 4/10\n55/55 [==============================] - 42s 757ms/step - loss: 0.0848 - balanced_accuracy: 0.9744 - val_loss: 0.3699 - val_balanced_accuracy: 0.9045\nEpoch 5/10\n55/55 [==============================] - 41s 749ms/step - loss: 0.0499 - balanced_accuracy: 0.9872 - val_loss: 0.3609 - val_balanced_accuracy: 0.9081\nEpoch 6/10\n55/55 [==============================] - 41s 746ms/step - loss: 0.0540 - balanced_accuracy: 0.9836 - val_loss: 0.3945 - val_balanced_accuracy: 0.9033\nEpoch 7/10\n55/55 [==============================] - 41s 749ms/step - loss: 0.0388 - balanced_accuracy: 0.9903 - val_loss: 0.4049 - val_balanced_accuracy: 0.9081\nEpoch 8/10\n55/55 [==============================] - 41s 755ms/step - loss: 0.0288 - balanced_accuracy: 0.9923 - val_loss: 0.4133 - val_balanced_accuracy: 0.9045\nEpoch 9/10\n55/55 [==============================] - 42s 764ms/step - loss: 0.0222 - balanced_accuracy: 0.9944 - val_loss: 0.4494 - val_balanced_accuracy: 0.9033\nEpoch 10/10\n55/55 [==============================] - 42s 763ms/step - loss: 0.0163 - balanced_accuracy: 0.9959 - val_loss: 0.4457 - val_balanced_accuracy: 0.9117\n              precision    recall  f1-score   support\n\n           0       0.92      0.90      0.91      1998\n           1       0.88      0.79      0.83      1639\n           2       0.94      0.80      0.87      6550\n           3       0.57      0.86      0.69      1499\n           4       0.92      0.93      0.92      4936\n           5       0.60      0.96      0.74       592\n\n    accuracy                           0.86     17214\n   macro avg       0.80      0.87      0.83     17214\nweighted avg       0.88      0.86      0.86     17214\n\n[[1804   17   32   23  120    2]\n [  41 1292   37   10   60  199]\n [  37   66 5270  856  182  139]\n [  13    5  146 1291   39    5]\n [  71   76  103   83 4571   32]\n [   2   12    3    5    3  567]]\n#################### 2 ###########################\n89\n(17124, 2) (2876, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2876 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18dc3a6157ba4d4db4313e3c7f91b673"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2876 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efadcba2d1bd4424a425d6ddf76c6710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2876 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d531153fd6e40f0be15ca657157e0c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2876 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627ba8ba7b9d498c9c69381ce80faf8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2876 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591fc82bd87248bba1a674ddcf97f4e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2876 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc6dfabb1a941c393f0577bf9547bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4381be01e74361b933980cde4c6ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eace78343f64f9da21d2f54f00398da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3a1678b8754593a2fab4bb0edd8fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"470944aa1fae4e1fb7c013407ec7262e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad5094e509c406a9dc19588b4cf5a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23019e3d685e4211a91d4100033600e2"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n56/56 [==============================] - 44s 778ms/step - loss: 0.3144 - balanced_accuracy: 0.9062 - val_loss: 0.2565 - val_balanced_accuracy: 0.9178\nEpoch 2/10\n56/56 [==============================] - 42s 760ms/step - loss: 0.1006 - balanced_accuracy: 0.9717 - val_loss: 0.3471 - val_balanced_accuracy: 0.9062\nEpoch 3/10\n56/56 [==============================] - 42s 751ms/step - loss: 0.0651 - balanced_accuracy: 0.9846 - val_loss: 0.2705 - val_balanced_accuracy: 0.9236\nEpoch 4/10\n56/56 [==============================] - 42s 757ms/step - loss: 0.0295 - balanced_accuracy: 0.9911 - val_loss: 0.3101 - val_balanced_accuracy: 0.9271\nEpoch 5/10\n56/56 [==============================] - 43s 770ms/step - loss: 0.0185 - balanced_accuracy: 0.9926 - val_loss: 0.3069 - val_balanced_accuracy: 0.9306\nEpoch 6/10\n56/56 [==============================] - 43s 768ms/step - loss: 0.0170 - balanced_accuracy: 0.9960 - val_loss: 0.3530 - val_balanced_accuracy: 0.9271\nEpoch 7/10\n56/56 [==============================] - 43s 767ms/step - loss: 0.0159 - balanced_accuracy: 0.9945 - val_loss: 0.3655 - val_balanced_accuracy: 0.9167\nEpoch 8/10\n56/56 [==============================] - 43s 771ms/step - loss: 0.0095 - balanced_accuracy: 0.9970 - val_loss: 0.3506 - val_balanced_accuracy: 0.9236\nEpoch 9/10\n56/56 [==============================] - 44s 795ms/step - loss: 0.0080 - balanced_accuracy: 0.9980 - val_loss: 0.3451 - val_balanced_accuracy: 0.9259\nEpoch 10/10\n56/56 [==============================] - 43s 778ms/step - loss: 0.0071 - balanced_accuracy: 0.9975 - val_loss: 0.3811 - val_balanced_accuracy: 0.9271\n              precision    recall  f1-score   support\n\n           0       0.93      0.91      0.92      1964\n           1       0.83      0.80      0.82      1619\n           2       0.96      0.81      0.88      6544\n           3       0.64      0.88      0.74      1498\n           4       0.91      0.94      0.93      4910\n           5       0.61      0.97      0.75       590\n\n    accuracy                           0.87     17125\n   macro avg       0.81      0.89      0.84     17125\nweighted avg       0.89      0.87      0.88     17125\n\n[[1779   31   30   12  109    3]\n [  24 1303   38    2   48  204]\n [  43  115 5329  687  239  131]\n [  11   12  103 1323   42    7]\n [  63  108   61   29 4633   16]\n [   0    7    4    3    4  572]]\n#################### 3 ###########################\n48\n(17076, 2) (2924, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2924 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11c7cdcc49f45e98ef0d2206036ae41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2924 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3105a2c9f2b840bea05a90552f292648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2924 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa625ee607e4e2496e6100a43873232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2924 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fcde42c5dd4210851e097a2e0ecee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2924 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59332acc945b422abf3993c257989797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2924 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e4b8ff5d3a46f589184a1277960d16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7d6af08e7b4e81802c1f3405295006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3d4940c2ba42c1a899f37f4600231a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087fb4cd74ef42509f31881d75b58943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f4a1b495714414bef2abf4843c4fbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937d0b6ed9464bc9bdb872c9de778bbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f79f7582634abc9bc3b1271cd56211"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n57/57 [==============================] - 45s 787ms/step - loss: 0.2519 - balanced_accuracy: 0.9433 - val_loss: 0.2479 - val_balanced_accuracy: 0.9363\nEpoch 2/10\n57/57 [==============================] - 43s 761ms/step - loss: 0.0759 - balanced_accuracy: 0.9800 - val_loss: 0.2937 - val_balanced_accuracy: 0.9329\nEpoch 3/10\n57/57 [==============================] - 44s 768ms/step - loss: 0.0456 - balanced_accuracy: 0.9878 - val_loss: 0.2340 - val_balanced_accuracy: 0.9431\nEpoch 4/10\n57/57 [==============================] - 44s 779ms/step - loss: 0.0256 - balanced_accuracy: 0.9946 - val_loss: 0.2458 - val_balanced_accuracy: 0.9431\nEpoch 5/10\n57/57 [==============================] - 44s 780ms/step - loss: 0.0174 - balanced_accuracy: 0.9971 - val_loss: 0.2538 - val_balanced_accuracy: 0.9408\nEpoch 6/10\n57/57 [==============================] - 44s 763ms/step - loss: 0.0143 - balanced_accuracy: 0.9956 - val_loss: 0.2817 - val_balanced_accuracy: 0.9420\nEpoch 7/10\n57/57 [==============================] - 44s 775ms/step - loss: 0.0089 - balanced_accuracy: 0.9971 - val_loss: 0.2894 - val_balanced_accuracy: 0.9443\nEpoch 8/10\n57/57 [==============================] - 44s 778ms/step - loss: 0.0080 - balanced_accuracy: 0.9980 - val_loss: 0.2983 - val_balanced_accuracy: 0.9397\nEpoch 9/10\n57/57 [==============================] - 44s 772ms/step - loss: 0.0146 - balanced_accuracy: 0.9956 - val_loss: 0.2958 - val_balanced_accuracy: 0.9408\nEpoch 10/10\n57/57 [==============================] - 43s 761ms/step - loss: 0.0080 - balanced_accuracy: 0.9976 - val_loss: 0.2987 - val_balanced_accuracy: 0.9408\n              precision    recall  f1-score   support\n\n           0       0.92      0.93      0.92      1946\n           1       0.90      0.77      0.83      1616\n           2       0.96      0.85      0.90      6536\n           3       0.71      0.88      0.78      1490\n           4       0.91      0.96      0.93      4899\n           5       0.64      0.98      0.78       590\n\n    accuracy                           0.89     17077\n   macro avg       0.84      0.89      0.86     17077\nweighted avg       0.90      0.89      0.89     17077\n\n[[1801   21   22    5   96    1]\n [  26 1242   29    1  101  217]\n [  34   61 5588  524  243   86]\n [  17    2  130 1307   30    4]\n [  70   59   50   14 4694   12]\n [   1    1    2    1    4  581]]\n#################### 4 ###########################\n19\n(17057, 2) (2943, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67fca9bfe98e4cb2a8e9badedfc49a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8590806d604c809e74c70f00a1d3de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d382fca9fa834783933be61cdb31a22c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b105c802fa39471e93cb1dee8aae6d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16f62ad24c540e08c099720d0492aca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c4122bff934dea9d661c6a3d3be6fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26dc89a503a747af8baab9b1f86d81f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b026a28f2d214260b6928a2fc2923c94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0212342d388647299ba2df48d1fc8ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b380f876794431b505a20c8330279a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ce4652b4183452d8c9987e6e9747c3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcc7ab2ea7c4e1f9e57babad298e98f"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n58/58 [==============================] - 45s 776ms/step - loss: 0.2257 - balanced_accuracy: 0.9481 - val_loss: 0.2979 - val_balanced_accuracy: 0.9423\nEpoch 2/10\n58/58 [==============================] - 43s 744ms/step - loss: 0.0618 - balanced_accuracy: 0.9884 - val_loss: 0.3168 - val_balanced_accuracy: 0.9367\nEpoch 3/10\n58/58 [==============================] - 44s 763ms/step - loss: 0.0316 - balanced_accuracy: 0.9956 - val_loss: 0.3143 - val_balanced_accuracy: 0.9446\nEpoch 4/10\n58/58 [==============================] - 43s 745ms/step - loss: 0.0261 - balanced_accuracy: 0.9951 - val_loss: 0.2953 - val_balanced_accuracy: 0.9400\nEpoch 5/10\n58/58 [==============================] - 45s 771ms/step - loss: 0.0116 - balanced_accuracy: 0.9981 - val_loss: 0.3042 - val_balanced_accuracy: 0.9446\nEpoch 6/10\n58/58 [==============================] - 44s 764ms/step - loss: 0.0136 - balanced_accuracy: 0.9966 - val_loss: 0.3911 - val_balanced_accuracy: 0.9434\nEpoch 7/10\n58/58 [==============================] - 43s 743ms/step - loss: 0.0107 - balanced_accuracy: 0.9966 - val_loss: 0.3458 - val_balanced_accuracy: 0.9446\nEpoch 8/10\n58/58 [==============================] - 45s 781ms/step - loss: 0.0089 - balanced_accuracy: 0.9971 - val_loss: 0.4012 - val_balanced_accuracy: 0.9457\nEpoch 9/10\n58/58 [==============================] - 44s 752ms/step - loss: 0.0074 - balanced_accuracy: 0.9985 - val_loss: 0.3633 - val_balanced_accuracy: 0.9491\nEpoch 10/10\n58/58 [==============================] - 44s 755ms/step - loss: 0.0070 - balanced_accuracy: 0.9995 - val_loss: 0.3705 - val_balanced_accuracy: 0.9457\n              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.93      1937\n           1       0.86      0.79      0.82      1611\n           2       0.96      0.85      0.90      6532\n           3       0.68      0.89      0.77      1490\n           4       0.92      0.95      0.94      4898\n           5       0.64      0.98      0.78       590\n\n    accuracy                           0.89     17058\n   macro avg       0.84      0.90      0.86     17058\nweighted avg       0.90      0.89      0.89     17058\n\n[[1792   24   26    5   88    2]\n [  26 1269   26    5   75  210]\n [  14  113 5530  579  204   92]\n [  14    5  122 1329   17    3]\n [  60   57   67   33 4668   13]\n [   0    1    3    1    5  580]]\n#################### 5 ###########################\n75\n(16982, 2) (3018, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f95027761b48d7bdbb3d96b2a4fd54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed8aa0a83aa4f40a068fec912ebc608"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65afa47d268546368160dc73b3b1c464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0df52b38cf455cb46d0b2f3114a717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37019dcc49b449ea8066aa5e6cc44ee5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d116ea06e944f6e844138cac3c0eb88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16982 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"239806169642411d8f3af5441d189dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16982 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d635e85de4c4f6dafb245d354f28003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16982 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2c852157564f50ba12aa1aa8571bb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16982 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6e36600215474fb4492589dde6248b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16982 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d28716e18d0425aa21c6ed3a2e79fa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16982 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821d88d6d01842e89f23d634aac26fe7"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n59/59 [==============================] - 46s 774ms/step - loss: 0.2541 - balanced_accuracy: 0.9290 - val_loss: 0.2504 - val_balanced_accuracy: 0.9427\nEpoch 2/10\n59/59 [==============================] - 45s 760ms/step - loss: 0.0619 - balanced_accuracy: 0.9872 - val_loss: 0.3247 - val_balanced_accuracy: 0.9338\nEpoch 3/10\n59/59 [==============================] - 45s 763ms/step - loss: 0.0283 - balanced_accuracy: 0.9920 - val_loss: 0.3233 - val_balanced_accuracy: 0.9372\nEpoch 4/10\n59/59 [==============================] - 45s 764ms/step - loss: 0.0258 - balanced_accuracy: 0.9929 - val_loss: 0.3810 - val_balanced_accuracy: 0.9350\nEpoch 5/10\n59/59 [==============================] - 45s 760ms/step - loss: 0.0186 - balanced_accuracy: 0.9948 - val_loss: 0.3706 - val_balanced_accuracy: 0.9383\nEpoch 6/10\n59/59 [==============================] - 45s 756ms/step - loss: 0.0107 - balanced_accuracy: 0.9967 - val_loss: 0.3348 - val_balanced_accuracy: 0.9438\nEpoch 7/10\n59/59 [==============================] - 45s 764ms/step - loss: 0.0097 - balanced_accuracy: 0.9967 - val_loss: 0.3877 - val_balanced_accuracy: 0.9394\nEpoch 8/10\n59/59 [==============================] - 45s 756ms/step - loss: 0.0085 - balanced_accuracy: 0.9976 - val_loss: 0.3620 - val_balanced_accuracy: 0.9405\nEpoch 9/10\n59/59 [==============================] - 45s 762ms/step - loss: 0.0069 - balanced_accuracy: 0.9976 - val_loss: 0.3614 - val_balanced_accuracy: 0.9416\nEpoch 10/10\n59/59 [==============================] - 45s 769ms/step - loss: 0.0067 - balanced_accuracy: 0.9976 - val_loss: 0.3731 - val_balanced_accuracy: 0.9383\n              precision    recall  f1-score   support\n\n           0       0.96      0.92      0.94      1918\n           1       0.91      0.80      0.85      1601\n           2       0.96      0.89      0.92      6510\n           3       0.73      0.93      0.81      1481\n           4       0.94      0.95      0.94      4884\n           5       0.65      0.98      0.78       589\n\n    accuracy                           0.91     16983\n   macro avg       0.86      0.91      0.88     16983\nweighted avg       0.92      0.91      0.91     16983\n\n[[1758   15   35    4  104    2]\n [  19 1279   22    1   67  213]\n [   8   41 5797  477  107   80]\n [   1    3   92 1370   13    2]\n [  41   71  102   27 4627   16]\n [   0    1    6    2    2  578]]\n#################### 6 ###########################\n44\n(16938, 2) (3062, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3062 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a987476df8254fa386b6cce0f6f84e7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3062 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d339d5f67ee04d9592b4048de85ab679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3062 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1263b4c2128f4c42b306e86af38cdb42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3062 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb4fde54bd944918ad3f1ece1d86cadb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3062 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f331150d79ac43158af81c312040b3f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3062 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ebd54c668b4426add0be9614d24610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ff04c81daf4180953c880e8501f149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0978fae5f6c14e22b39f647c3f48cb09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d981ad2b9a40c7b29110c33aeedfe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef5b05b409d4b7c94bcae69c32f838e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45010ee7cfba4c94aa54bfa82279b5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16938 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d110eac6d84f13a0ec5561a7ae5c42"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n60/60 [==============================] - 46s 763ms/step - loss: 0.1532 - balanced_accuracy: 0.9632 - val_loss: 0.2304 - val_balanced_accuracy: 0.9467\nEpoch 2/10\n60/60 [==============================] - 45s 753ms/step - loss: 0.0418 - balanced_accuracy: 0.9902 - val_loss: 0.2413 - val_balanced_accuracy: 0.9435\nEpoch 3/10\n60/60 [==============================] - 45s 753ms/step - loss: 0.0200 - balanced_accuracy: 0.9963 - val_loss: 0.2352 - val_balanced_accuracy: 0.9522\nEpoch 4/10\n60/60 [==============================] - 45s 756ms/step - loss: 0.0145 - balanced_accuracy: 0.9949 - val_loss: 0.2526 - val_balanced_accuracy: 0.9467\nEpoch 5/10\n60/60 [==============================] - 46s 760ms/step - loss: 0.0193 - balanced_accuracy: 0.9939 - val_loss: 0.2570 - val_balanced_accuracy: 0.9457\nEpoch 6/10\n60/60 [==============================] - 45s 747ms/step - loss: 0.0094 - balanced_accuracy: 0.9967 - val_loss: 0.2574 - val_balanced_accuracy: 0.9522\nEpoch 7/10\n60/60 [==============================] - 46s 761ms/step - loss: 0.0075 - balanced_accuracy: 0.9972 - val_loss: 0.2575 - val_balanced_accuracy: 0.9478\nEpoch 8/10\n60/60 [==============================] - 45s 750ms/step - loss: 0.0075 - balanced_accuracy: 0.9967 - val_loss: 0.2578 - val_balanced_accuracy: 0.9533\nEpoch 9/10\n60/60 [==============================] - 46s 768ms/step - loss: 0.0057 - balanced_accuracy: 0.9977 - val_loss: 0.2574 - val_balanced_accuracy: 0.9533\nEpoch 10/10\n60/60 [==============================] - 46s 762ms/step - loss: 0.0048 - balanced_accuracy: 0.9977 - val_loss: 0.2629 - val_balanced_accuracy: 0.9533\n              precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      1900\n           1       0.84      0.84      0.84      1588\n           2       0.95      0.88      0.92      6505\n           3       0.72      0.89      0.80      1481\n           4       0.96      0.94      0.95      4876\n           5       0.67      0.98      0.79       589\n\n    accuracy                           0.90     16939\n   macro avg       0.85      0.91      0.87     16939\nweighted avg       0.91      0.90      0.91     16939\n\n[[1768   15   32    4   81    0]\n [  19 1330   18    2   21  198]\n [  15  128 5732  472   80   78]\n [   8    5  132 1317   16    3]\n [  45   97   99   25 4600   10]\n [   0    6    6    0    1  576]]\n#################### 7 ###########################\n11\n(16927, 2) (3073, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3073 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33853be281ca467c990e3f7d456ab410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3073 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237167b0ed6944ed99b6f7cc61a45754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3073 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c323a98f1824318991c366e76b7bf23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3073 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cd80a99941349cf8556e4010dffefcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3073 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd8c10fb3d141559f852cbf2490e6c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3073 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cd5dbead204f21b1607c73100bf794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304381bff12343b18859b93881a00e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f0a2a2ddf84291a51d54850a54fdc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af81e5894e7044ca97646c43a5735418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b54e031644f44cbb6b273e9e656b740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f9530862a64268ae9639cc340b122d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16927 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821182930eb44693a3885d709b55e16c"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n60/60 [==============================] - 46s 760ms/step - loss: 0.1555 - balanced_accuracy: 0.9633 - val_loss: 0.2336 - val_balanced_accuracy: 0.9512\nEpoch 2/10\n60/60 [==============================] - 45s 748ms/step - loss: 0.0463 - balanced_accuracy: 0.9893 - val_loss: 0.2305 - val_balanced_accuracy: 0.9567\nEpoch 3/10\n60/60 [==============================] - 46s 763ms/step - loss: 0.0342 - balanced_accuracy: 0.9926 - val_loss: 0.2338 - val_balanced_accuracy: 0.9577\nEpoch 4/10\n60/60 [==============================] - 46s 768ms/step - loss: 0.0159 - balanced_accuracy: 0.9963 - val_loss: 0.2638 - val_balanced_accuracy: 0.9599\nEpoch 5/10\n60/60 [==============================] - 46s 763ms/step - loss: 0.0112 - balanced_accuracy: 0.9967 - val_loss: 0.2719 - val_balanced_accuracy: 0.9588\nEpoch 6/10\n60/60 [==============================] - 46s 764ms/step - loss: 0.0113 - balanced_accuracy: 0.9967 - val_loss: 0.2756 - val_balanced_accuracy: 0.9588\nEpoch 7/10\n60/60 [==============================] - 45s 756ms/step - loss: 0.0068 - balanced_accuracy: 0.9977 - val_loss: 0.2887 - val_balanced_accuracy: 0.9567\nEpoch 8/10\n60/60 [==============================] - 45s 757ms/step - loss: 0.0074 - balanced_accuracy: 0.9963 - val_loss: 0.2889 - val_balanced_accuracy: 0.9577\nEpoch 9/10\n60/60 [==============================] - 45s 749ms/step - loss: 0.0070 - balanced_accuracy: 0.9977 - val_loss: 0.2869 - val_balanced_accuracy: 0.9599\nEpoch 10/10\n60/60 [==============================] - 45s 746ms/step - loss: 0.0048 - balanced_accuracy: 0.9977 - val_loss: 0.2834 - val_balanced_accuracy: 0.9599\n              precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      1897\n           1       0.83      0.85      0.84      1586\n           2       0.96      0.86      0.91      6504\n           3       0.74      0.90      0.81      1478\n           4       0.94      0.94      0.94      4874\n           5       0.63      0.97      0.76       589\n\n    accuracy                           0.90     16928\n   macro avg       0.84      0.91      0.87     16928\nweighted avg       0.91      0.90      0.90     16928\n\n[[1768   17   23    2   83    4]\n [  20 1348   10    0   16  192]\n [  14  136 5620  444  176  114]\n [   7    8  109 1337   13    4]\n [  45  107   76   21 4601   24]\n [   0   12    1    1    2  573]]\n#################### 8 ###########################\n36\n(16891, 2) (3109, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618ba0bd4c9c48e0b0995e24f216a81a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635fb7e14d93473f88041e1125243969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad8d62cd7ac34651bb6a85b276e81441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ae59aefe3f43c0b89a66e8f16b973a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c76b5e5c002435bbcddbe46b1849656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff29f3ddee94ddcb72a610636936524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16891 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3b78657c04433bb4855ed8927d1127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16891 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768e079db05f499c93c6441f85255a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16891 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76142a6f4764e9eb04761180e785bb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16891 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ddd46b32b824155a9fcf3fd1b86b2f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16891 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"181a691468f9473f9abac0dc2617b28b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16891 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c32c1cfa44fb40a78a3bf2e959876f0c"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n61/61 [==============================] - 47s 769ms/step - loss: 0.2095 - balanced_accuracy: 0.9486 - val_loss: 0.2611 - val_balanced_accuracy: 0.9486\nEpoch 2/10\n61/61 [==============================] - 46s 749ms/step - loss: 0.0302 - balanced_accuracy: 0.9931 - val_loss: 0.2269 - val_balanced_accuracy: 0.9518\nEpoch 3/10\n61/61 [==============================] - 46s 756ms/step - loss: 0.0154 - balanced_accuracy: 0.9963 - val_loss: 0.2528 - val_balanced_accuracy: 0.9550\nEpoch 4/10\n61/61 [==============================] - 46s 750ms/step - loss: 0.0064 - balanced_accuracy: 0.9982 - val_loss: 0.2459 - val_balanced_accuracy: 0.9561\nEpoch 5/10\n61/61 [==============================] - 46s 755ms/step - loss: 0.0045 - balanced_accuracy: 0.9982 - val_loss: 0.2456 - val_balanced_accuracy: 0.9561\nEpoch 6/10\n61/61 [==============================] - 45s 738ms/step - loss: 0.0055 - balanced_accuracy: 0.9982 - val_loss: 0.2523 - val_balanced_accuracy: 0.9550\nEpoch 7/10\n61/61 [==============================] - 46s 751ms/step - loss: 0.0056 - balanced_accuracy: 0.9991 - val_loss: 0.2569 - val_balanced_accuracy: 0.9582\nEpoch 8/10\n61/61 [==============================] - 46s 757ms/step - loss: 0.0060 - balanced_accuracy: 0.9982 - val_loss: 0.2689 - val_balanced_accuracy: 0.9582\nEpoch 9/10\n61/61 [==============================] - 47s 766ms/step - loss: 0.0038 - balanced_accuracy: 0.9982 - val_loss: 0.2651 - val_balanced_accuracy: 0.9582\nEpoch 10/10\n61/61 [==============================] - 46s 758ms/step - loss: 0.0063 - balanced_accuracy: 0.9982 - val_loss: 0.2775 - val_balanced_accuracy: 0.9561\n              precision    recall  f1-score   support\n\n           0       0.96      0.93      0.95      1877\n           1       0.91      0.81      0.86      1584\n           2       0.98      0.87      0.92      6495\n           3       0.72      0.92      0.81      1478\n           4       0.93      0.97      0.95      4869\n           5       0.65      0.98      0.78       589\n\n    accuracy                           0.91     16892\n   macro avg       0.86      0.91      0.88     16892\nweighted avg       0.92      0.91      0.91     16892\n\n[[1750   14   17    3   90    3]\n [  21 1283   14    0   56  210]\n [  14   24 5677  506  189   85]\n [   4    4   73 1364   32    1]\n [  32   78   29   10 4706   14]\n [   0    3    4    2    4  576]]\n#################### 9 ###########################\n158\n(16733, 2) (3267, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ffa932af544964aa1e15610653b135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe1ae3b63524db1a7ef4110958c7634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa60bd15e4a45cd89698ab2ee254b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d30f641b626443b869e8d283c1045d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125031e858b24e39842b4ce7892a6cfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3267 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea4fef7db4142cb96ec3ba225fd96cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf796da7a824af1b5f6d4ad3cedc774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4b2c869c924354af5ec839aca8a582"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d37264acd34047a97d06ce40d3b960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae1fed944c64e6293dae5ecd94af2e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e4fcbf5b76242b2888cf8f73c1c560c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697bc0a28f5e49e38856cb5c6f703051"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n64/64 [==============================] - 50s 779ms/step - loss: 0.2268 - balanced_accuracy: 0.9458 - val_loss: 0.2465 - val_balanced_accuracy: 0.9470\nEpoch 2/10\n64/64 [==============================] - 50s 776ms/step - loss: 0.0659 - balanced_accuracy: 0.9843 - val_loss: 0.2210 - val_balanced_accuracy: 0.9593\nEpoch 3/10\n64/64 [==============================] - 50s 780ms/step - loss: 0.0289 - balanced_accuracy: 0.9921 - val_loss: 0.2437 - val_balanced_accuracy: 0.9552\nEpoch 4/10\n64/64 [==============================] - 48s 757ms/step - loss: 0.0167 - balanced_accuracy: 0.9952 - val_loss: 0.2488 - val_balanced_accuracy: 0.9562\nEpoch 5/10\n64/64 [==============================] - 48s 758ms/step - loss: 0.0183 - balanced_accuracy: 0.9956 - val_loss: 0.2565 - val_balanced_accuracy: 0.9562\nEpoch 6/10\n64/64 [==============================] - 48s 752ms/step - loss: 0.0111 - balanced_accuracy: 0.9961 - val_loss: 0.2516 - val_balanced_accuracy: 0.9562\nEpoch 7/10\n64/64 [==============================] - 49s 761ms/step - loss: 0.0128 - balanced_accuracy: 0.9948 - val_loss: 0.2709 - val_balanced_accuracy: 0.9532\nEpoch 8/10\n64/64 [==============================] - 49s 762ms/step - loss: 0.0115 - balanced_accuracy: 0.9969 - val_loss: 0.2589 - val_balanced_accuracy: 0.9603\nEpoch 9/10\n64/64 [==============================] - 49s 761ms/step - loss: 0.0091 - balanced_accuracy: 0.9956 - val_loss: 0.2590 - val_balanced_accuracy: 0.9562\nEpoch 10/10\n64/64 [==============================] - 49s 760ms/step - loss: 0.0087 - balanced_accuracy: 0.9965 - val_loss: 0.2613 - val_balanced_accuracy: 0.9582\n              precision    recall  f1-score   support\n\n           0       0.97      0.94      0.95      1853\n           1       0.91      0.82      0.87      1582\n           2       0.96      0.90      0.93      6481\n           3       0.77      0.89      0.83      1477\n           4       0.95      0.96      0.96      4752\n           5       0.64      0.98      0.78       589\n\n    accuracy                           0.92     16734\n   macro avg       0.87      0.92      0.88     16734\nweighted avg       0.93      0.92      0.92     16734\n\n[[1736   16   22    4   73    2]\n [  19 1302   12    1   36  212]\n [   5   29 5863  387  105   92]\n [   4    2  129 1318   19    5]\n [  29   75   59    6 4568   15]\n [   0    2    5    1    1  580]]\n#################### 10 ###########################\n9\n(16724, 2) (3276, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3276 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03541d3b60974d13b818ebbe38cfa6d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3276 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d50b4eb484fc443bbd530a514080a813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3276 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"054f308570d444128b243a925979cb1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3276 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e57353127c04cfa80a9c37aaeb19661"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3276 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f759b6f3c4ad4c2294503c149008643d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3276 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b48b50fd7920489690c66e6e5cf0d8e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8029230530824c0284ddf230408d4693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530ea2e6e5664cb9b02d8ab146aaa054"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f34dfffa0d54d7bbe0525b17d93a85f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cf5e1bb2e2c466cbd9f3cbaac2dda9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcab909334dc458a8be64c22cda0968c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16724 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb3168fae6ec437b99deb5e25be18baf"}},"metadata":{}},{"name":"stdout","text":"The number of words in string are : 115 565\nEpoch 1/10\n64/64 [==============================] - 50s 772ms/step - loss: 0.1730 - balanced_accuracy: 0.9573 - val_loss: 0.2381 - val_balanced_accuracy: 0.9573\nEpoch 2/10\n64/64 [==============================] - 49s 770ms/step - loss: 0.0468 - balanced_accuracy: 0.9900 - val_loss: 0.2417 - val_balanced_accuracy: 0.9593\nEpoch 3/10\n64/64 [==============================] - 50s 777ms/step - loss: 0.0153 - balanced_accuracy: 0.9969 - val_loss: 0.2684 - val_balanced_accuracy: 0.9553\nEpoch 4/10\n64/64 [==============================] - 50s 784ms/step - loss: 0.0143 - balanced_accuracy: 0.9969 - val_loss: 0.2500 - val_balanced_accuracy: 0.9604\nEpoch 5/10\n64/64 [==============================] - 50s 783ms/step - loss: 0.0110 - balanced_accuracy: 0.9965 - val_loss: 0.2736 - val_balanced_accuracy: 0.9563\nEpoch 6/10\n64/64 [==============================] - 50s 783ms/step - loss: 0.0104 - balanced_accuracy: 0.9974 - val_loss: 0.2762 - val_balanced_accuracy: 0.9522\nEpoch 7/10\n64/64 [==============================] - 50s 782ms/step - loss: 0.0058 - balanced_accuracy: 0.9983 - val_loss: 0.2635 - val_balanced_accuracy: 0.9604\nEpoch 8/10\n64/64 [==============================] - 50s 782ms/step - loss: 0.0066 - balanced_accuracy: 0.9974 - val_loss: 0.2734 - val_balanced_accuracy: 0.9624\nEpoch 9/10\n64/64 [==============================] - 50s 788ms/step - loss: 0.0056 - balanced_accuracy: 0.9978 - val_loss: 0.2756 - val_balanced_accuracy: 0.9614\nEpoch 10/10\n64/64 [==============================] - 50s 782ms/step - loss: 0.0058 - balanced_accuracy: 0.9974 - val_loss: 0.2694 - val_balanced_accuracy: 0.9614\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95      1850\n           1       0.91      0.83      0.87      1581\n           2       0.97      0.89      0.93      6477\n           3       0.73      0.92      0.81      1477\n           4       0.96      0.96      0.96      4751\n           5       0.66      0.99      0.79       589\n\n    accuracy                           0.92     16725\n   macro avg       0.86      0.92      0.88     16725\nweighted avg       0.93      0.92      0.92     16725\n\n[[1736   15   23    4   70    2]\n [  21 1307   22    1   25  205]\n [   5   30 5794  488   82   78]\n [   5    3   85 1362   19    3]\n [  33   79   63   13 4550   13]\n [   0    1    2    1    4  581]]\n#################### 11 ###########################\n1401\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3411479326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataTx_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlis3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/2366871320.py\u001b[0m in \u001b[0;36mdataTx_trial\u001b[0;34m(txlist, df_test1, df_train1)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataTx_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdf_train1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdf_test1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"],"ename":"IndexError","evalue":"single positional indexer is out-of-bounds","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}